{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BELOW CODE SECTION IS A ROUGH SECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT INSTRCUTIONS\n",
    "\n",
    "1. Use as minimum variables as needed\n",
    "2. try to reuse exsting variables.\n",
    "3. Delete the variables if not used to save the RAM memory\n",
    "4. try to define and use functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import gc\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n",
    "\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# smote = SMOTE()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    }
   ],
   "source": [
    "print(len(dir()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FOLDER PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT            = Path(\"E:\\\\Semester-2\\\\DS ML\\\\Group-Project\\\\Data_Home_Credit\\\\\")\n",
    "\n",
    "TRAIN_DIR       = ROOT / \"parquet_files\" / \"train\"\n",
    "TEST_DIR        = ROOT / \"parquet_files\" / \"test\"\n",
    "\n",
    "#to store intermediate results, instead of keeping them in RAM\n",
    "TRAIN_INTER       = ROOT / \"parquet_files\" / \"train_inter\"\n",
    "TEST_INTER        = ROOT / \"parquet_files\" / \"test_inter\"\n",
    "\n",
    "#to store intermediate results, instead of keeping them in RAM\n",
    "TRAIN_INTER1       = ROOT / \"parquet_files\" / \"train_inter1\"\n",
    "TEST_INTER1        = ROOT / \"parquet_files\" / \"test_inter1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PIPELINE CLASS - Has functions for the following use cases\n",
    "1. to set the datatypes of the table\n",
    "2. to handle dates --- yet to understand\n",
    "3. to filter the columns --- yet to understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE - The Pipeline functions here are used on the polars dataframe\n",
    "class Pipeline:\n",
    "\n",
    "    #PENDING -- Handle cases for T,L\n",
    "    def set_table_dtypes(df):\n",
    "        for col in df.columns:\n",
    "            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Int64))\n",
    "            elif col in [\"date_decision\"]:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Date))\n",
    "            elif col[-1] in (\"P\", \"A\"):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Int64))\n",
    "            elif col[-1] in (\"M\",):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.String))\n",
    "            elif col[-1] in (\"D\",):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Date))\n",
    "        return df\n",
    "\n",
    "    def handle_dates(df): #PENDING, CHECK ALL THE DATES\n",
    "        for col in df.columns:\n",
    "            if col[-1] in (\"D\",):\n",
    "                df = df.with_columns(pl.col(col) - pl.col(\"date_decision\")) #!!?\n",
    "                df = df.with_columns(pl.col(col).dt.total_days().cast(pl.Int64)) # t - t-1\n",
    "        df = df.drop(\"date_decision\", \"MONTH\")\n",
    "        return df\n",
    "\n",
    "    #pass a list as argument and dont drop the columns if present in the list\n",
    "    def filter_cols(df, cols_must_present=[]):\n",
    "        for col in df.columns:\n",
    "            if (col not in [\"target\", \"case_id\", \"WEEK_NUM\"]) and (col not in cols_must_present):\n",
    "                isnull = df[col].is_null().mean()\n",
    "                if (isnull!= None) and (isnull > 0.7):\n",
    "                    df = df.drop(col)\n",
    "\n",
    "        #uncomment later\n",
    "        # for col in df.columns:\n",
    "        #     if (col not in [\"target\", \"case_id\", \"WEEK_NUM\"]) & (df[col].dtype == pl.String):\n",
    "        #         freq = df[col].n_unique()\n",
    "        #         if (freq == 1) | (freq > 200):\n",
    "        #             df = df.drop(col)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "\n",
    "\n",
    "    #Group together columns based on the correlation, and select the top column from each group\n",
    "    def filter_correlated_columns(df):\n",
    "        #reduce the number of groups\n",
    "        def reduce_group(grps):\n",
    "            use = []\n",
    "            for g in grps:\n",
    "                mx = 0; vx = g[0]\n",
    "                for gg in g:\n",
    "                    n = df[gg].n_unique()\n",
    "                    if n>mx:\n",
    "                        mx = n\n",
    "                        vx = gg\n",
    "                    #print(str(gg)+'-'+str(n),', ',end='')\n",
    "                use.append(vx)\n",
    "                #print()\n",
    "            # print('Use these',use)\n",
    "            return use\n",
    "        \n",
    "        def group_columns_by_correlation(matrix, threshold=0.8):\n",
    "            matrix=matrix.drop_nulls()\n",
    "            # print('matrix',matrix)\n",
    "            correlation_matrix = matrix.corr()\n",
    "\n",
    "            # print('correlation_matrix',correlation_matrix)\n",
    "\n",
    "            # dict to store 'columnname' as key and index as 'value'\n",
    "            index_dict = {}\n",
    "\n",
    "            # Iterate through the list and fill the dictionary\n",
    "            for index, element in enumerate(matrix.columns):\n",
    "                index_dict[element] = index\n",
    "\n",
    "            # print('index dict',index_dict)\n",
    "\n",
    "            groups = []\n",
    "            remaining_cols = list(matrix.columns)\n",
    "            # print('correlation matrix',correlation_matrix)\n",
    "\n",
    "            while remaining_cols:\n",
    "                col = remaining_cols.pop(0)\n",
    "                group = [col]\n",
    "                correlated_cols = [col]\n",
    "                \n",
    "                for c in remaining_cols:\n",
    "                    row = index_dict[c]\n",
    "                    corr_value = correlation_matrix.select(pl.col(col))\n",
    "                    corr_value = corr_value[col][row]\n",
    "                    # print('corr_value',corr_value)\n",
    "                    if corr_value >= threshold:\n",
    "                        group.append(c)\n",
    "                        correlated_cols.append(c)\n",
    "                groups.append(group)\n",
    "                remaining_cols = [c for c in remaining_cols if c not in correlated_cols]\n",
    "            \n",
    "            return groups\n",
    "\n",
    "        #exclude string/object datatpyes - add more to the list like object Boolean\n",
    "        df_changed = df.select(pl.exclude([pl.String,pl.Date]))\n",
    "\n",
    "        #create a masked polar dataframe for df_changed\n",
    "        df_nans = {}\n",
    "        for col in df_changed.columns:\n",
    "            df_nans[col] = df_changed[col].is_null()\n",
    "        df_nans = pl.DataFrame(df_nans)\n",
    "        # print(df_nans)\n",
    "\n",
    "        #dict - key= number of missing values,, value=list having column names\n",
    "        nans_groups={}\n",
    "        for col in df_nans.columns:\n",
    "            cur_group = df_nans[col].sum()\n",
    "            try:\n",
    "                nans_groups[cur_group].append(col)\n",
    "            except:\n",
    "                nans_groups[cur_group]=[col]\n",
    "\n",
    "        #uses is a list which have the selected columns for the model\n",
    "        uses = []\n",
    "        for k,v in nans_groups.items():\n",
    "            if len(v)>1:\n",
    "                    # print('level 1')\n",
    "                    Vs = nans_groups[k]\n",
    "                    grps= group_columns_by_correlation(df[Vs], threshold=0.8)\n",
    "\n",
    "                    # print('grps',grps)\n",
    "                    use=reduce_group(grps)\n",
    "                    uses=uses+use\n",
    "            else:\n",
    "                uses=uses+v\n",
    "            print('####### NAN count =',k)\n",
    "        \n",
    "\n",
    "        #add the columns which were excluded earlier, while computing the correlation\n",
    "        # Define the data types you want to select\n",
    "        data_types_to_select = ['String', 'Date']  # Adjust data types as needed like Boolean, matchin the upper list\n",
    "        # Get column names of selected data types\n",
    "        excluded_cats = [col for col in df.columns if str(df[col].dtype) in data_types_to_select]\n",
    "        uses=uses+excluded_cats\n",
    "\n",
    "        print('columns that are not dropped after correlation are')\n",
    "        print(uses)\n",
    "\n",
    "        return df.select(uses)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def drop_rows_with_nulls(df):\n",
    "        df = df.drop_nulls()\n",
    "        return df\n",
    "    \n",
    "    def normalize_cols(df):\n",
    "   \n",
    "        def normalize_column(column):\n",
    "            min_val = column.min()\n",
    "            max_val = column.max()\n",
    "            return (column - min_val) / (max_val - min_val)\n",
    "        \n",
    "        # Apply Min-Max scaling to each numerical column (both Float64 and Int64)\n",
    "        normalized_df = df.select([\n",
    "            normalize_column(pl.col(col)).alias(col) \n",
    "            if df[col].dtype in [pl.Float64, pl.Int64] and col not in ['MONTH', 'num_group2', 'num_group1', 'date_decision', 'WEEK_NUM', 'target', 'case_id']\n",
    "            else pl.col(col)\n",
    "            for col in df.columns\n",
    "        ])\n",
    "        \n",
    "        return normalized_df\n",
    "    \n",
    "    #Instead of encode columns use, cat_column and then pass them as categories using pandas\n",
    "    # X_train[cat_cols].astype(\"category\") --- used for pandas\n",
    "    def encode_cols(df):\n",
    "        for col in df.columns:\n",
    "            dict1={}\n",
    "            freq = df[col].n_unique()\n",
    "            categories = df[col].unique().to_list()\n",
    "            if freq < 200 and df[col].dtype == pl.String:\n",
    "                for i in range(freq):\n",
    "                    dict1[categories[i]]=i\n",
    "\n",
    "                new_col_name = col\n",
    "\n",
    "\n",
    "                df = df.with_columns( \n",
    "                        df[col].map_elements(lambda x: dict1.get(x), return_dtype=pl.UInt8).alias(new_col_name)\n",
    "                )\n",
    "\n",
    "            # print(dict1)\n",
    "        return df\n",
    "\n",
    "    #this function should fill the missing values with a valid metric\n",
    "    def handle_missing_values(df):\n",
    "        pass\n",
    "    \n",
    "                \n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "    \n",
    "# print(sys.getsizeof(Pipeline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AGGREGATOR CLASS -- HAS functions to return the list of functions expressions that are passed as parameter to 'agg' function which follows 'group_by' function.\n",
    "1. number expressions\n",
    "2. date expressions\n",
    "3. string expressions\n",
    "4. Object expressions\n",
    "5. group expressions(having depth=1 and depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Aggregator:\n",
    "    #Please add or subtract features yourself, be aware that too many features will take up too much space.\n",
    "    def num_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"P\", \"A\")]\n",
    "        #expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        \n",
    "        #expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n",
    "        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n",
    "        expr_mean = [pl.mean(col).cast(pl.Int64).alias(f\"mean_{col}\") for col in cols]\n",
    "\n",
    "\n",
    "        # expr_myMetric = [((pl.median(col) + pl.mean(col) + 2*pl.max(col))/4).cast(pl.Int64).alias(f\"myMetric_{col}\") for col in cols]\n",
    "\n",
    "        return expr_mean #+expr_myMetric #+ expr_max #+expr_last\n",
    "    \n",
    "    def date_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"D\")]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        #expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n",
    "        # expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n",
    "        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n",
    "        # expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]\n",
    "\n",
    "        return  expr_max #+ expr_last +expr_mean\n",
    "    \n",
    "    #take max for string data types\n",
    "    def str_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"M\",)]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        #expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n",
    "        # expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n",
    "        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n",
    "        #expr_count = [pl.count(col).alias(f\"count_{col}\") for col in cols]\n",
    "        return  expr_max #+expr_last#+expr_count\n",
    "    \n",
    "    def other_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"T\", \"L\")]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        #expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n",
    "        # expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n",
    "        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n",
    "        return  expr_max #+expr_last\n",
    "    \n",
    "    def count_expr(df):\n",
    "        cols = [col for col in df.columns if \"num_group\" in col]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols] \n",
    "        #expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n",
    "        # expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n",
    "        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n",
    "        return  expr_max #+expr_last\n",
    "    \n",
    "    def get_exprs(df):\n",
    "        exprs = Aggregator.num_expr(df) + \\\n",
    "                Aggregator.date_expr(df) + \\\n",
    "                Aggregator.str_expr(df) + \\\n",
    "                Aggregator.other_expr(df) + \\\n",
    "                Aggregator.count_expr(df)\n",
    "\n",
    "        return exprs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "READ FILE/ READ FILES\n",
    "1. read_file -- reads single file\n",
    "2. read_files -- reads group of similar files together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path, depth=None):\n",
    "    df = pl.read_parquet(path)\n",
    "    df = df.pipe(Pipeline.set_table_dtypes)\n",
    "    if depth in [1,2]:\n",
    "        df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df)) \n",
    "    return df\n",
    "\n",
    "def read_files(regex_path, depth=None):\n",
    "    chunks = []\n",
    "    \n",
    "    for path in glob(str(regex_path)):\n",
    "        df = pl.read_parquet(path)\n",
    "        df = df.pipe(Pipeline.set_table_dtypes)\n",
    "        if depth in [1, 2]:\n",
    "            df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n",
    "        chunks.append(df)\n",
    "    \n",
    "    df = pl.concat(chunks, how=\"vertical_relaxed\")\n",
    "    df = df.unique(subset=[\"case_id\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUNCTION TO REDUCE THE SIZE OF DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage_polars(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.estimated_size() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = str(df[col].dtype)\n",
    "        # print(col,'coltype before',col_type)\n",
    "        if  col_type in  ['Int64', 'Float64']:\n",
    "            #perform below operations\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "\n",
    "            if (c_max is None) or (c_min is None):\n",
    "                print(\"heyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\")\n",
    "                continue\n",
    "\n",
    "            if col_type == 'Int64' or col_type == 'Float64':\n",
    "                # print('reduce mem if condition')\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df = df.with_columns(pl.col(col).cast(pl.Int8))\n",
    "                    # print('reduce mem if condition - Int8')\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df = df.with_columns(pl.col(col).cast(pl.Int16))\n",
    "                    # print('reduce mem if condition Int16')\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df = df.with_columns(pl.col(col).cast(pl.Int32))\n",
    "                    # print('reduce mem if condition Int32')\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df = df.with_columns(pl.col(col).cast(pl.Int64))\n",
    "                    # print('reduce mem if condition Int64')\n",
    "\n",
    "            # elif col_type == 'Float64':\n",
    "            #     if c_max < np.finfo(np.float32).max:\n",
    "            #         df = df.with_columns(pl.col(col).cast(pl.Float32))\n",
    "            #     else:\n",
    "            #         df = df.with_columns(pl.col(col).cast(pl.Float64))\n",
    "        # print(col,'coltype after',df[col].dtype)\n",
    "        # print('\\n')\n",
    "\n",
    "    end_mem = df.estimated_size() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE A DATA STORE FORMAT -- To perform READ/WRITE for TRAIN DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_store_train(path):\n",
    "    d = {\n",
    "        \"df_base\": [\n",
    "            [read_file , path / \"train_base.parquet\"],\n",
    "        ],\n",
    "\n",
    "        \"depth_0\": [\n",
    "            [read_file , path / \"train_static_cb_0.parquet\"],\n",
    "            [read_files , path / \"train_static_0_*.parquet\"],\n",
    "        ],\n",
    "\n",
    "        \"depth_1\": [\n",
    "\n",
    "            [read_files , path / \"train_applprev_1_*.parquet\", 1],\n",
    "            [read_file , path / \"train_tax_registry_a_1.parquet\", 1],\n",
    "            [read_file , path / \"train_tax_registry_b_1.parquet\", 1],\n",
    "            [read_file , path / \"train_tax_registry_c_1.parquet\", 1],\n",
    "            [read_files , path / \"train_credit_bureau_a_1_*.parquet\", 1],\n",
    "            [read_file , path / \"train_credit_bureau_b_1.parquet\", 1],\n",
    "            [read_file , path / \"train_other_1.parquet\", 1],\n",
    "            [read_file , path / \"train_person_1.parquet\", 1],\n",
    "            [read_file , path / \"train_deposit_1.parquet\", 1],\n",
    "            [read_file , path / \"train_debitcard_1.parquet\", 1],\n",
    "        ],\n",
    "\n",
    "\n",
    "        \"depth_2\": [\n",
    "            [read_file , path / \"train_credit_bureau_b_2.parquet\", 2],\n",
    "            [read_files , path / \"train_credit_bureau_a_2_*.parquet\", 2],\n",
    "            [read_file , path / \"train_applprev_2.parquet\", 2],\n",
    "            [read_file , path / \"train_person_2.parquet\", 2],\n",
    "        ]\n",
    "\n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184\n"
     ]
    }
   ],
   "source": [
    "data_store = create_data_store_train(TRAIN_DIR)\n",
    "\n",
    "\n",
    "# print(data_store['df_base'])\n",
    "\n",
    "print(sys.getsizeof(data_store))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INITIAL OPERATIONS ON THE PARQUET FILES - BOTH TRAIN AND TEST DATA\n",
    "1. Read parquet files\n",
    "2. Modify the dataframe accordingly\n",
    "3. Store the intermediate files in seperate folder, to save RAM space\n",
    "4. Below function can be used for both train/test folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def updateParquetFiles(data_store,cols_must_present=[]):\n",
    "    for value in data_store.values():\n",
    "        for item in value:\n",
    "            # print(item[1]) -- corresponds to the path of the parquet_file\n",
    "\n",
    "            if len(item)==2:\n",
    "                df = item[0](item[1]) #readfile/ readfiles happens here\n",
    "            elif len(item)==3:\n",
    "                df = item[0](item[1],item[2]) #readfile/ readfiles happens here\n",
    "            \n",
    "            #skip the empty file\n",
    "            if df.shape[0] == 0:\n",
    "                continue\n",
    "\n",
    "            #filetering the colunmns of dataframe\n",
    "            # 1. you can perform filter_cols here\n",
    "            # 2. you can perform filter_cols in feature engineering\n",
    "            df = df.pipe(Pipeline.filter_cols,cols_must_present)\n",
    "\n",
    "            #dropping the rows with missing values -- store this result in {folder_name}_inter1\n",
    "            # df = df.pipe(Pipeline.drop_rows_with_nulls)\n",
    "            #INSTEAD of dropping rows having the null values \n",
    "                # 1. retain the missing values/and handle it later - This method is selected\n",
    "                # 2. impute the missing values -- it is better to fill missing values after joining all tables/files\n",
    "\n",
    "            #reduce the dataframe memory size\n",
    "            df = df.pipe(reduce_mem_usage_polars)\n",
    "\n",
    "            #WRITE df as parquet file to ../train_inter/ directory.\n",
    "            # 1. Get the filename from the path\n",
    "            filename = os.path.basename(item[1])\n",
    "            filename = filename.replace(\"_*\",\"\")\n",
    "            # Get the folder name from the path\n",
    "            folder_name = os.path.basename(os.path.dirname(item[1]))\n",
    "\n",
    "            # 2. Go one folder back\n",
    "            parent_directory = os.path.dirname(os.path.dirname(item[1]))\n",
    "\n",
    "            # 3. Create directory '{folder_name}_inter1' if not present\n",
    "            new_directory = os.path.join(parent_directory, f\"{folder_name}_inter1\")\n",
    "            if not os.path.exists(new_directory):\n",
    "                os.makedirs(new_directory)\n",
    "\n",
    "            # 4. Create new path\n",
    "            new_path = os.path.join(new_directory, filename)\n",
    "\n",
    "            # Write the DataFrame to a Parquet file in the new path\n",
    "            df.write_parquet(new_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UPDATE TRAIN PARQUET FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 52.41 MB\n",
      "Memory usage after optimization is: 20.38 MB\n",
      "Decreased by 61.1%\n",
      "\n",
      "\n",
      "Memory usage of dataframe is 238.64 MB\n",
      "Memory usage after optimization is: 117.01 MB\n",
      "Decreased by 51.0%\n",
      "\n",
      "\n",
      "Memory usage of dataframe is 1600.85 MB\n",
      "Memory usage after optimization is: 571.50 MB\n",
      "Decreased by 64.3%\n",
      "\n",
      "\n",
      "Memory usage of dataframe is 261.33 MB\n",
      "Memory usage after optimization is: 172.80 MB\n",
      "Decreased by 33.9%\n",
      "\n",
      "\n",
      "Memory usage of dataframe is 15.75 MB\n",
      "Memory usage after optimization is: 9.20 MB\n",
      "Decreased by 41.6%\n",
      "\n",
      "\n",
      "Memory usage of dataframe is 5.19 MB\n",
      "Memory usage after optimization is: 3.03 MB\n",
      "Decreased by 41.6%\n",
      "\n",
      "\n",
      "Memory usage of dataframe is 16.58 MB\n",
      "Memory usage after optimization is: 9.68 MB\n",
      "Decreased by 41.6%\n",
      "\n",
      "\n",
      "Memory usage of dataframe is 707.80 MB\n",
      "Memory usage after optimization is: 391.83 MB\n",
      "Decreased by 44.6%\n",
      "\n",
      "\n",
      "Memory usage of dataframe is 11.69 MB\n",
      "Memory usage after optimization is: 6.54 MB\n",
      "Decreased by 44.1%\n",
      "\n",
      "\n",
      "Memory usage of dataframe is 2.73 MB\n",
      "Memory usage after optimization is: 1.22 MB\n",
      "Decreased by 55.4%\n",
      "\n",
      "\n",
      "Memory usage of dataframe is 254.33 MB\n",
      "Memory usage after optimization is: 201.92 MB\n",
      "Decreased by 20.6%\n",
      "\n",
      "\n",
      "Memory usage of dataframe is 3.22 MB\n",
      "Memory usage after optimization is: 1.72 MB\n",
      "Decreased by 46.7%\n",
      "\n",
      "\n",
      "Memory usage of dataframe is 2.15 MB\n",
      "Memory usage after optimization is: 0.97 MB\n",
      "Decreased by 54.7%\n",
      "\n",
      "\n",
      "Memory usage of dataframe is 1.54 MB\n",
      "Memory usage after optimization is: 0.56 MB\n",
      "Decreased by 63.3%\n",
      "\n",
      "\n",
      "Memory usage of dataframe is 202.46 MB\n",
      "Memory usage after optimization is: 119.23 MB\n",
      "Decreased by 41.1%\n",
      "\n",
      "\n",
      "Memory usage of dataframe is 53.39 MB\n",
      "Memory usage after optimization is: 32.42 MB\n",
      "Decreased by 39.3%\n",
      "\n",
      "\n",
      "Memory usage of dataframe is 87.63 MB\n",
      "Memory usage after optimization is: 62.99 MB\n",
      "Decreased by 28.1%\n",
      "\n",
      "\n",
      "CPU times: total: 8min 25s\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "updateParquetFiles(data_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FEATURE ENGINEERING DEFINITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def feature_eng(df_base, depth_0, depth_1, depth_2):\n",
    "\n",
    "    # print(df_base)\n",
    "    folderPath = os.path.dirname(os.path.dirname(df_base[0][1]))\n",
    "    folderName = os.path.basename(os.path.dirname(df_base[0][1]))\n",
    "\n",
    "    outputFolderName = folderName.replace(\"inter1\",\"inter\")\n",
    "    outputFolderPath = os.path.join(folderPath,outputFolderName)\n",
    "\n",
    "    # print('outputFolderPath',outputFolderPath)\n",
    "\n",
    "\n",
    "    #reading the parquet file using polars\n",
    "    df_base = pl.read_parquet(df_base[0][1])\n",
    "\n",
    "    #adding new columns to df_base\n",
    "    df_base = df_base.with_columns(\n",
    "        month_decision = pl.col(\"date_decision\").dt.month().cast(pl.Int8),\n",
    "        weekday_decision = pl.col(\"date_decision\").dt.weekday().cast(pl.Int8),\n",
    "    )\n",
    "    # print(df_base)\n",
    "\n",
    "    for i, list1 in enumerate(depth_0 + depth_1 + depth_2):\n",
    "        filename = str(list1[1])\n",
    "        \n",
    "        filename = filename.replace(\"_*\",\"\")\n",
    "        # print(filename)\n",
    "\n",
    "        if os.path.exists(filename):\n",
    "            df = pl.read_parquet(filename)\n",
    "            df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "    # #filter columns based on  the percentage of null values in each column - uncomment if needed\n",
    "    # df_base = df_base.pipe(Pipeline.filter_cols)\n",
    "\n",
    "    # filter_correlated_columns\n",
    "    df_base = df_base.pipe(Pipeline.filter_correlated_columns)\n",
    "    \n",
    "\n",
    "    #handle rows with missing values\n",
    "    # 1. drop rows with missing values\n",
    "    # 2. fill the missing values using Imputers\n",
    "\n",
    "    #dropping rows with missing values - comment later\n",
    "    # df_base = df_base.pipe(Pipeline.drop_rows_with_nulls)\n",
    "        \n",
    "    outputFilename = os.path.join(outputFolderPath,'joined.parquet')\n",
    "    print('outputfilename',outputFilename)\n",
    "\n",
    "    #create director if not present\n",
    "    if not os.path.exists(outputFolderPath):\n",
    "        # Create the directory\n",
    "        os.makedirs(outputFolderPath)\n",
    "\n",
    "    df_base = df_base.pipe(Pipeline.handle_dates)\n",
    "\n",
    "    #writing the joined.parquet file to the outputFilename\n",
    "    df_base.write_parquet(outputFilename)\n",
    "\n",
    "    \n",
    "    return df_base\n",
    "\n",
    "    # print('size of df_base',df_base)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FEATURE ENGINEERING ON TRAIN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kmoha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\kmoha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### NAN count = 0\n",
      "####### NAN count = 140968\n",
      "####### NAN count = 954021\n",
      "####### NAN count = 418178\n",
      "####### NAN count = 561124\n",
      "####### NAN count = 4\n",
      "####### NAN count = 613202\n",
      "####### NAN count = 948244\n",
      "####### NAN count = 972827\n",
      "####### NAN count = 467175\n",
      "####### NAN count = 624875\n",
      "####### NAN count = 757006\n",
      "####### NAN count = 841181\n",
      "####### NAN count = 1026987\n",
      "####### NAN count = 455190\n",
      "####### NAN count = 460822\n",
      "####### NAN count = 343375\n",
      "####### NAN count = 452594\n",
      "####### NAN count = 190833\n",
      "####### NAN count = 442041\n",
      "####### NAN count = 769046\n",
      "####### NAN count = 511255\n",
      "####### NAN count = 306019\n",
      "####### NAN count = 960953\n",
      "####### NAN count = 705504\n",
      "####### NAN count = 876276\n",
      "####### NAN count = 829402\n",
      "####### NAN count = 1032856\n",
      "####### NAN count = 766958\n",
      "####### NAN count = 452593\n",
      "####### NAN count = 455081\n",
      "####### NAN count = 445669\n",
      "####### NAN count = 456495\n",
      "####### NAN count = 847191\n",
      "####### NAN count = 446983\n",
      "####### NAN count = 840646\n",
      "####### NAN count = 669186\n",
      "####### NAN count = 455612\n",
      "####### NAN count = 305137\n",
      "####### NAN count = 458738\n",
      "####### NAN count = 461362\n",
      "####### NAN count = 459827\n",
      "####### NAN count = 460079\n",
      "####### NAN count = 44954\n",
      "####### NAN count = 78526\n",
      "####### NAN count = 131888\n",
      "####### NAN count = 181122\n",
      "####### NAN count = 223240\n",
      "####### NAN count = 445320\n",
      "####### NAN count = 3\n",
      "####### NAN count = 305154\n",
      "####### NAN count = 308739\n",
      "####### NAN count = 307441\n",
      "####### NAN count = 419006\n",
      "####### NAN count = 306361\n",
      "####### NAN count = 450969\n",
      "####### NAN count = 420383\n",
      "####### NAN count = 961606\n",
      "####### NAN count = 552766\n",
      "####### NAN count = 321446\n",
      "####### NAN count = 1068725\n",
      "####### NAN count = 1375927\n",
      "####### NAN count = 1044394\n",
      "####### NAN count = 1036944\n",
      "####### NAN count = 603001\n",
      "####### NAN count = 263166\n",
      "####### NAN count = 514070\n",
      "####### NAN count = 606920\n",
      "####### NAN count = 263233\n",
      "####### NAN count = 517511\n",
      "####### NAN count = 545885\n",
      "####### NAN count = 636453\n",
      "####### NAN count = 512650\n",
      "####### NAN count = 263171\n",
      "####### NAN count = 262653\n",
      "####### NAN count = 512590\n",
      "####### NAN count = 513987\n",
      "####### NAN count = 1039597\n",
      "####### NAN count = 606900\n",
      "####### NAN count = 545855\n",
      "####### NAN count = 636448\n",
      "####### NAN count = 297072\n",
      "####### NAN count = 822517\n",
      "####### NAN count = 745109\n",
      "####### NAN count = 545898\n",
      "####### NAN count = 636545\n",
      "####### NAN count = 545895\n",
      "####### NAN count = 636544\n",
      "####### NAN count = 512657\n",
      "####### NAN count = 561307\n",
      "####### NAN count = 649082\n",
      "####### NAN count = 140386\n",
      "####### NAN count = 1501222\n",
      "####### NAN count = 1499078\n",
      "####### NAN count = 1510533\n",
      "####### NAN count = 1490245\n",
      "####### NAN count = 1490212\n",
      "####### NAN count = 1495366\n",
      "####### NAN count = 1490244\n",
      "####### NAN count = 1501266\n",
      "####### NAN count = 1510534\n",
      "####### NAN count = 1499241\n",
      "####### NAN count = 1506562\n",
      "####### NAN count = 1501223\n",
      "####### NAN count = 1490159\n",
      "####### NAN count = 1475550\n",
      "####### NAN count = 441\n",
      "####### NAN count = 935626\n",
      "####### NAN count = 2\n",
      "####### NAN count = 1421548\n",
      "####### NAN count = 1414887\n",
      "####### NAN count = 262659\n",
      "####### NAN count = 512884\n",
      "####### NAN count = 512598\n",
      "####### NAN count = 141371\n",
      "####### NAN count = 91554\n",
      "columns that are not dropped after correlation are\n",
      "['case_id', 'WEEK_NUM', 'target', 'month_decision', 'weekday_decision', 'credamount_770A', 'applicationcnt_361L', 'applications30d_658L', 'applicationscnt_1086L', 'applicationscnt_464L', 'applicationscnt_867L', 'clientscnt_1022L', 'clientscnt_100L', 'clientscnt_1071L', 'clientscnt_1130L', 'clientscnt_157L', 'clientscnt_257L', 'clientscnt_304L', 'clientscnt_360L', 'clientscnt_493L', 'clientscnt_533L', 'clientscnt_887L', 'clientscnt_946L', 'deferredmnthsnum_166L', 'disbursedcredamount_1113A', 'downpmt_116A', 'homephncnt_628L', 'isbidproduct_1095L', 'mobilephncnt_593L', 'numactivecreds_622L', 'numactivecredschannel_414L', 'numactiverelcontr_750L', 'numcontrs3months_479L', 'numnotactivated_1143L', 'numpmtchanneldd_318L', 'numrejects9m_859L', 'sellerplacecnt_915L', 'mean_mainoccupationinc_384A', 'max_num_group1_9', 'days180_256L', 'days30_165L', 'days360_512L', 'firstquarter_103L', 'fourthquarter_440L', 'secondquarter_766L', 'thirdquarter_1082L', 'mean_debtoutstand_525A', 'mean_debtoverdue_47A', 'pmtscount_423L', 'pmtssum_45A', 'actualdpdtolerance_344P', 'amtinstpaidbefduel24m_4187115A', 'numinstlswithdpd5_4187116L', 'annuitynextmonth_57A', 'currdebt_22A', 'currdebtcredtyperange_828A', 'numinstls_657L', 'totalsettled_863A', 'mindbddpdlast24m_3658935P', 'avgdbddpdlast3m_4187120P', 'avgdbdtollast24m_4525197P', 'avgdpdtolclosure24_3658938P', 'avginstallast24m_3658937A', 'maxinstallast24m_3658928A', 'avgmaxdpdlast9m_3716943P', 'avgoutstandbalancel6m_4187114A', 'avgpmtlast12m_4525200A', 'cntincpaycont9m_3716944L', 'cntpmts24_3658933L', 'commnoinclast6m_3546845L', 'maxdpdfrom6mto36m_3546853P', 'daysoverduetolerancedd_3976961L', 'numinsttopaygr_769L', 'eir_270L', 'interestrate_311L', 'lastapprcredamount_781A', 'lastrejectcredamount_222A', 'maininc_215A', 'mastercontrelectronic_519L', 'mastercontrexist_109L', 'maxannuity_159A', 'maxdebt4_972A', 'maxdpdlast24m_143P', 'maxdpdlast3m_392P', 'maxdpdtolerance_374P', 'maxdbddpdlast1m_3658939P', 'maxdbddpdtollast12m_3658940P', 'maxdbddpdtollast6m_4187119P', 'maxdpdinstlnum_3546846P', 'maxlnamtstart6m_4525199A', 'maxoutstandbalancel12m_4187113A', 'numinstpaidearly_338L', 'numinstpaidearly5d_1087L', 'numinstpaidlate1d_3546852L', 'numincomingpmts_3546848L', 'numinstlsallpaid_934L', 'numinstlswithdpd10_728L', 'numinstlswithoutdpd_562L', 'numinstpaid_4499208L', 'numinstpaidearly3d_3546850L', 'numinstregularpaidest_4493210L', 'numinstpaidearly5dest_4493211L', 'sumoutstandtotalest_4493215A', 'numinstpaidlastcontr_4325080L', 'numinstregularpaid_973L', 'opencred_647L', 'max_isbidproduct_390L', 'max_num_group1', 'max_num_group2_14', 'pctinstlsallpaidearl3d_427L', 'pctinstlsallpaidlate1d_3546856L', 'pctinstlsallpaidlat10d_839L', 'pctinstlsallpaidlate4d_3546849L', 'pctinstlsallpaidlate6d_3546844L', 'pmtnum_254L', 'posfpd10lastmonth_333P', 'posfpd30lastmonth_3976960P', 'posfstqpd30lastmonth_3976962P', 'price_1097A', 'sumoutstandtotal_3546847A', 'totaldebt_9A', 'mean_actualdpd_943P', 'mean_annuity_853A', 'mean_credacc_credlmt_575A', 'mean_credamount_590A', 'mean_downpmt_134A', 'mean_currdebt_94A', 'mean_mainoccupationinc_437A', 'mean_maxdpdtolerance_577P', 'mean_outstandingdebt_522A', 'max_byoccupationinc_3656910L', 'max_childnum_21L', 'max_pmtnum_8L', 'mean_amount_4527230A', 'max_num_group1_3', 'mean_amount_4917619A', 'max_num_group1_4', 'mean_pmtamount_36A', 'max_num_group1_5', 'mean_credlmt_230A', 'mean_credlmt_935A', 'mean_dpdmax_139P', 'max_dpdmaxdatemonth_89T', 'max_dpdmaxdateyear_596T', 'mean_dpdmax_757P', 'max_dpdmaxdatemonth_442T', 'max_dpdmaxdateyear_896T', 'mean_pmts_dpd_303P', 'mean_instlamount_768A', 'mean_monthlyinstlamount_332A', 'mean_monthlyinstlamount_674A', 'mean_outstandingamount_354A', 'mean_outstandingamount_362A', 'mean_overdueamount_31A', 'mean_overdueamount_659A', 'max_numberofoverdueinstls_725L', 'mean_overdueamountmax2_14A', 'mean_totaldebtoverduevalue_178A', 'mean_totaloutstanddebtvalue_39A', 'max_numberofcontrsvalue_258L', 'max_numberofoverdueinstlmax_1039L', 'max_overdueamountmaxdatemonth_365T', 'max_overdueamountmaxdateyear_2T', 'mean_pmts_overdue_1140A', 'max_pmts_month_158T', 'max_pmts_year_1139T', 'mean_overdueamountmax2_398A', 'max_numberofoverdueinstlmax_1151L', 'mean_overdueamountmax_35A', 'max_overdueamountmaxdatemonth_284T', 'max_overdueamountmaxdateyear_994T', 'mean_pmts_overdue_1152A', 'mean_residualamount_488A', 'mean_residualamount_856A', 'mean_totalamount_6A', 'mean_totalamount_996A', 'mean_totaldebtoverduevalue_718A', 'mean_totaloutstanddebtvalue_668A', 'max_numberofcontrsvalue_358L', 'max_nominalrate_281L', 'max_nominalrate_498L', 'max_numberofinstls_229L', 'max_numberofinstls_320L', 'max_numberofoutstandinstls_520L', 'max_numberofoutstandinstls_59L', 'max_numberofoverdueinstls_834L', 'max_periodicityofpmts_1102L', 'max_periodicityofpmts_837L', 'max_num_group1_6', 'mean_debtvalue_227A', 'mean_credlmt_1052A', 'mean_credlmt_3940954A', 'mean_residualamount_127A', 'mean_credlmt_228A', 'mean_debtpastduevalue_732A', 'mean_pmtdaysoverdue_1135P', 'mean_dpd_550P', 'mean_installmentamount_833A', 'mean_totalamount_503A', 'max_num_group1_12', 'max_num_group2', 'mean_installmentamount_644A', 'mean_totalamount_881A', 'max_credquantity_984L', 'mean_dpdmax_851P', 'mean_overdueamountmax_950A', 'max_dpdmaxdatemonth_804T', 'max_dpdmaxdateyear_742T', 'mean_pmts_dpdvalue_108P', 'mean_instlamount_892A', 'max_numberofinstls_810L', 'mean_residualamount_1093A', 'mean_residualamount_3940956A', 'max_interestrateyearly_538L', 'max_pmtnumpending_403L', 'max_num_group1_7', 'mean_amtdebitoutgoing_4809440A', 'mean_amtdepositbalance_4809441A', 'mean_amtdepositoutgoing_4809442A', 'max_num_group1_8', 'max_contaddr_matchlist_1032L', 'max_contaddr_smempladdr_334L', 'max_remitter_829L', 'max_safeguarantyflag_411L', 'mean_amount_416A', 'max_num_group1_10', 'max_num_group1_11', 'max_collater_valueofguarantee_1124L', 'max_collater_valueofguarantee_876L', 'max_pmts_month_706T', 'max_pmts_year_507T', 'max_num_group1_13', 'max_num_group2_13', 'max_num_group1_15', 'max_num_group2_15', 'date_decision', 'birthdate_574D', 'dateofbirth_337D', 'description_5085714M', 'education_1103M', 'education_88M', 'maritalst_385M', 'maritalst_893M', 'requesttype_4525192L', 'responsedate_1012D', 'responsedate_4527233D', 'credtype_322L', 'datefirstoffer_1144D', 'datelastunpaid_3546854D', 'disbursementtype_67L', 'dtlastpmtallstes_4499206D', 'firstclxcampaign_1125D', 'firstdatedue_489D', 'inittransactioncode_186L', 'lastactivateddate_801D', 'lastapplicationdate_877D', 'lastapprcommoditycat_1041M', 'lastapprcommoditytypec_5251766M', 'lastapprdate_640D', 'lastcancelreason_561M', 'lastdelinqdate_224D', 'lastrejectcommoditycat_161M', 'lastrejectcommodtypec_5251769M', 'lastrejectdate_50D', 'lastrejectreason_759M', 'lastrejectreasonclient_4145040M', 'lastst_736L', 'maxdpdinstldate_3546855D', 'paytype1st_925L', 'paytype_783L', 'previouscontdistrict_112M', 'twobodfilling_608L', 'max_approvaldate_319D', 'max_creationdate_885D', 'max_dateactivated_425D', 'max_dtlastpmt_581D', 'max_dtlastpmtallstes_3545839D', 'max_employedfrom_700D', 'max_firstnonzeroinstldate_307D', 'max_cancelreason_3545846M', 'max_district_544M', 'max_education_1138M', 'max_postype_4733339M', 'max_profession_152M', 'max_rejectreason_755M', 'max_rejectreasonclient_4145042M', 'max_credtype_587L', 'max_familystate_726L', 'max_inittransactioncode_279L', 'max_status_219L', 'max_recorddate_4527225D', 'max_name_4527232M', 'max_deductiondate_4917603D', 'max_name_4917606M', 'max_processingdate_168D', 'max_employername_160M', 'max_dateofcredend_289D', 'max_dateofcredend_353D', 'max_dateofcredstart_181D', 'max_dateofcredstart_739D', 'max_dateofrealrepmt_138D', 'max_lastupdate_1112D', 'max_lastupdate_388D', 'max_numberofoverdueinstlmaxdat_148D', 'max_numberofoverdueinstlmaxdat_641D', 'max_overdueamountmax2date_1002D', 'max_overdueamountmax2date_1142D', 'max_refreshdate_3813885D', 'max_classificationofcontr_13M', 'max_classificationofcontr_400M', 'max_contractst_545M', 'max_contractst_964M', 'max_description_351M', 'max_financialinstitution_382M', 'max_financialinstitution_591M', 'max_purposeofcred_426M', 'max_purposeofcred_874M', 'max_subjectrole_182M', 'max_subjectrole_93M', 'max_contractdate_551D', 'max_contractmaturitydate_151D', 'max_lastupdate_260D', 'max_classificationofcontr_1114M', 'max_contractst_516M', 'max_contracttype_653M', 'max_credor_3940957M', 'max_periodicityofpmts_997M', 'max_pmtmethod_731M', 'max_purposeofcred_722M', 'max_subjectrole_326M', 'max_subjectrole_43M', 'max_birth_259D', 'max_empl_employedfrom_271D', 'max_contaddr_district_15M', 'max_contaddr_zipcode_807M', 'max_education_927M', 'max_empladdr_district_926M', 'max_empladdr_zipcode_114M', 'max_language1_981M', 'max_registaddr_district_1083M', 'max_registaddr_zipcode_184M', 'max_empl_employedtotal_800L', 'max_empl_industry_691L', 'max_familystate_447L', 'max_incometype_1044T', 'max_relationshiptoclient_415T', 'max_relationshiptoclient_642T', 'max_role_1084L', 'max_sex_738L', 'max_type_25L', 'max_contractenddate_991D', 'max_openingdate_313D', 'max_openingdate_857D', 'max_pmts_date_1107D', 'max_collater_typofvalofguarant_298M', 'max_collater_typofvalofguarant_407M', 'max_collaterals_typeofguarante_359M', 'max_collaterals_typeofguarante_669M', 'max_subjectroles_name_541M', 'max_subjectroles_name_838M', 'max_cacccardblochreas_147M', 'max_conts_type_509L', 'max_addres_district_368M', 'max_addres_zip_823M', 'max_conts_role_79M', 'max_empls_economicalst_849M', 'max_empls_employer_name_740M']\n",
      "outputfilename E:\\Semester-2\\DS ML\\Group-Project\\Data_Home_Credit\\parquet_files\\train_inter\\joined.parquet\n",
      "final shape of the dataframe to be given as input to ML Model (1526659, 362)\n"
     ]
    }
   ],
   "source": [
    "df_base = feature_eng(**create_data_store_train(TRAIN_INTER1))\n",
    "\n",
    "#drop unnecessary columns that are not required for training the model\n",
    "#PENDING -- pass an list as angument, dont drop the columns, if that is present in list\n",
    "def drop_columns(df):\n",
    "    cols_to_drop = [\"case_id\", \"date_decision\", \"MONTH\", \"WEEK_NUM\", \"month_decision\",\"weekday_decision\",\n",
    "                    #'dateofbirth_337D','max_birth_259D','max_contaddr_district_15M',\n",
    "                    #'max_contaddr_zipcode_807M', 'max_registaddr_district_1083M', 'max_registaddr_zipcode_184M', 'max_addres_district_368M', 'max_addres_zip_823M',\n",
    "                    #'max_num_group1_9', 'max_num_group1_15', 'max_num_group2_15', 'max_num_group2_14', 'max_num_group1_14', 'max_num_group2_13','max_num_group1_13'\n",
    "                    ]\n",
    "    df = df.drop(cols_to_drop)\n",
    "    return df\n",
    "df_base = drop_columns(df_base)\n",
    "\n",
    "\n",
    "# df_base = df_base.pipe(Pipeline.encode_cols)\n",
    "\n",
    "#test/submit data should have all the columns prsent in df_base.schema \n",
    "# to get the prediction from the model\n",
    "# print(df_base.schema)\n",
    "print('final shape of the dataframe to be given as input to ML Model',df_base.shape)\n",
    "\n",
    "#must have columns\n",
    "cols_must_present = list(df_base.schema.keys())\n",
    "# print(cols_must_present)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proportion(df):\n",
    "    y_counts = df['target'].value_counts()\n",
    "\n",
    "    # Calculate the proportion of each class\n",
    "    proportion_0 = y_counts[0] / len(df['target'])\n",
    "    proportion_1 = y_counts[1] / len(df['target'])\n",
    "\n",
    "    # Print the proportions\n",
    "    print(\"Proportion of class 0:\", round(proportion_0,2))\n",
    "    print(\"Proportion of class 1:\", round(proportion_1,2))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL TO BE TAKEN\n",
    "1. Light GBM \n",
    "2. Cat Boost\n",
    "3. Mix of 1 and 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LGBM - PARAMETERS INITIALIZATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#param initilization of light gbm\n",
    "\n",
    "params_lgbm = {\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"auc\",\n",
    "    \"max_depth\": 10,  \n",
    "    \"learning_rate\": 0.05,\n",
    "    \"n_estimators\": 2000,  \n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"colsample_bynode\": 0.8,\n",
    "    \"verbose\": -1,\n",
    "    \"random_state\": 42,\n",
    "    \"reg_alpha\": 0.1,\n",
    "    \"reg_lambda\": 10,\n",
    "    \"extra_trees\":True,\n",
    "    'num_leaves':64,\n",
    "    \"device\": 'cpu', \n",
    "    \"verbose\": -1,\n",
    "}\n",
    "\n",
    "fitted_models = []\n",
    "cv_scores = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA SAMPLERS INITIALIZATIONS - TO perform oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define stratified shuffle split\n",
    "\n",
    "smote = SMOTE()\n",
    "adasyn = ADASYN()\n",
    "random_oversampler = RandomOverSampler(random_state=42)\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(n_splits=4, shuffle=False) #tune this parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING THE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove below section later - just for testing purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target                            int8\n",
      "credamount_770A                  int32\n",
      "applicationcnt_361L               int8\n",
      "applications30d_658L              int8\n",
      "applicationscnt_1086L            int16\n",
      "                                 ...  \n",
      "max_addres_district_368M        object\n",
      "max_addres_zip_823M             object\n",
      "max_conts_role_79M              object\n",
      "max_empls_economicalst_849M     object\n",
      "max_empls_employer_name_740M    object\n",
      "Length: 362, dtype: object\n",
      "Index(['target', 'credamount_770A', 'applicationcnt_361L',\n",
      "       'applications30d_658L', 'applicationscnt_1086L', 'applicationscnt_464L',\n",
      "       'applicationscnt_867L', 'clientscnt_1022L', 'clientscnt_100L',\n",
      "       'clientscnt_1071L',\n",
      "       ...\n",
      "       'max_collaterals_typeofguarante_669M', 'max_subjectroles_name_541M',\n",
      "       'max_subjectroles_name_838M', 'max_cacccardblochreas_147M',\n",
      "       'max_conts_type_509L', 'max_addres_district_368M',\n",
      "       'max_addres_zip_823M', 'max_conts_role_79M',\n",
      "       'max_empls_economicalst_849M', 'max_empls_employer_name_740M'],\n",
      "      dtype='object', length=362)\n",
      "Index(['target', 'credamount_770A', 'applicationcnt_361L',\n",
      "       'applications30d_658L', 'applicationscnt_1086L', 'applicationscnt_464L',\n",
      "       'applicationscnt_867L', 'clientscnt_1022L', 'clientscnt_100L',\n",
      "       'clientscnt_1071L',\n",
      "       ...\n",
      "       'max_collaterals_typeofguarante_669M', 'max_subjectroles_name_541M',\n",
      "       'max_subjectroles_name_838M', 'max_cacccardblochreas_147M',\n",
      "       'max_conts_type_509L', 'max_addres_district_368M',\n",
      "       'max_addres_zip_823M', 'max_conts_role_79M',\n",
      "       'max_empls_economicalst_849M', 'max_empls_employer_name_740M'],\n",
      "      dtype='object', length=362)\n"
     ]
    }
   ],
   "source": [
    "df_base_pandas = df_base.to_pandas()\n",
    "\n",
    "print(df_base_pandas.dtypes)\n",
    "\n",
    "print(df_base_pandas.columns)\n",
    "\n",
    "print(df_base_pandas.select_dtypes(exclude='category').columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat_cols ['isbidproduct_1095L', 'opencred_647L', 'max_isbidproduct_390L', 'max_contaddr_matchlist_1032L', 'max_contaddr_smempladdr_334L', 'max_remitter_829L', 'max_safeguarantyflag_411L', 'description_5085714M', 'education_1103M', 'education_88M', 'maritalst_385M', 'maritalst_893M', 'requesttype_4525192L', 'credtype_322L', 'disbursementtype_67L', 'inittransactioncode_186L', 'lastapprcommoditycat_1041M', 'lastapprcommoditytypec_5251766M', 'lastcancelreason_561M', 'lastrejectcommoditycat_161M', 'lastrejectcommodtypec_5251769M', 'lastrejectreason_759M', 'lastrejectreasonclient_4145040M', 'lastst_736L', 'paytype1st_925L', 'paytype_783L', 'previouscontdistrict_112M', 'twobodfilling_608L', 'max_cancelreason_3545846M', 'max_district_544M', 'max_education_1138M', 'max_postype_4733339M', 'max_profession_152M', 'max_rejectreason_755M', 'max_rejectreasonclient_4145042M', 'max_credtype_587L', 'max_familystate_726L', 'max_inittransactioncode_279L', 'max_status_219L', 'max_name_4527232M', 'max_name_4917606M', 'max_employername_160M', 'max_classificationofcontr_13M', 'max_classificationofcontr_400M', 'max_contractst_545M', 'max_contractst_964M', 'max_description_351M', 'max_financialinstitution_382M', 'max_financialinstitution_591M', 'max_purposeofcred_426M', 'max_purposeofcred_874M', 'max_subjectrole_182M', 'max_subjectrole_93M', 'max_classificationofcontr_1114M', 'max_contractst_516M', 'max_contracttype_653M', 'max_credor_3940957M', 'max_periodicityofpmts_997M', 'max_pmtmethod_731M', 'max_purposeofcred_722M', 'max_subjectrole_326M', 'max_subjectrole_43M', 'max_contaddr_district_15M', 'max_contaddr_zipcode_807M', 'max_education_927M', 'max_empladdr_district_926M', 'max_empladdr_zipcode_114M', 'max_language1_981M', 'max_registaddr_district_1083M', 'max_registaddr_zipcode_184M', 'max_empl_employedtotal_800L', 'max_empl_industry_691L', 'max_familystate_447L', 'max_incometype_1044T', 'max_relationshiptoclient_415T', 'max_relationshiptoclient_642T', 'max_role_1084L', 'max_sex_738L', 'max_type_25L', 'max_collater_typofvalofguarant_298M', 'max_collater_typofvalofguarant_407M', 'max_collaterals_typeofguarante_359M', 'max_collaterals_typeofguarante_669M', 'max_subjectroles_name_541M', 'max_subjectroles_name_838M', 'max_cacccardblochreas_147M', 'max_conts_type_509L', 'max_addres_district_368M', 'max_addres_zip_823M', 'max_conts_role_79M', 'max_empls_economicalst_849M', 'max_empls_employer_name_740M']\n",
      "len of cat_cols 92\n",
      "num_cols ['target', 'credamount_770A', 'applicationcnt_361L', 'applications30d_658L', 'applicationscnt_1086L', 'applicationscnt_464L', 'applicationscnt_867L', 'clientscnt_1022L', 'clientscnt_100L', 'clientscnt_1071L', 'clientscnt_1130L', 'clientscnt_157L', 'clientscnt_257L', 'clientscnt_304L', 'clientscnt_360L', 'clientscnt_493L', 'clientscnt_533L', 'clientscnt_887L', 'clientscnt_946L', 'deferredmnthsnum_166L', 'disbursedcredamount_1113A', 'downpmt_116A', 'homephncnt_628L', 'mobilephncnt_593L', 'numactivecreds_622L', 'numactivecredschannel_414L', 'numactiverelcontr_750L', 'numcontrs3months_479L', 'numnotactivated_1143L', 'numpmtchanneldd_318L', 'numrejects9m_859L', 'sellerplacecnt_915L', 'mean_mainoccupationinc_384A', 'max_num_group1_9', 'days180_256L', 'days30_165L', 'days360_512L', 'firstquarter_103L', 'fourthquarter_440L', 'secondquarter_766L', 'thirdquarter_1082L', 'mean_debtoutstand_525A', 'mean_debtoverdue_47A', 'pmtscount_423L', 'pmtssum_45A', 'actualdpdtolerance_344P', 'amtinstpaidbefduel24m_4187115A', 'numinstlswithdpd5_4187116L', 'annuitynextmonth_57A', 'currdebt_22A', 'currdebtcredtyperange_828A', 'numinstls_657L', 'totalsettled_863A', 'mindbddpdlast24m_3658935P', 'avgdbddpdlast3m_4187120P', 'avgdbdtollast24m_4525197P', 'avgdpdtolclosure24_3658938P', 'avginstallast24m_3658937A', 'maxinstallast24m_3658928A', 'avgmaxdpdlast9m_3716943P', 'avgoutstandbalancel6m_4187114A', 'avgpmtlast12m_4525200A', 'cntincpaycont9m_3716944L', 'cntpmts24_3658933L', 'commnoinclast6m_3546845L', 'maxdpdfrom6mto36m_3546853P', 'daysoverduetolerancedd_3976961L', 'numinsttopaygr_769L', 'eir_270L', 'interestrate_311L', 'lastapprcredamount_781A', 'lastrejectcredamount_222A', 'maininc_215A', 'mastercontrelectronic_519L', 'mastercontrexist_109L', 'maxannuity_159A', 'maxdebt4_972A', 'maxdpdlast24m_143P', 'maxdpdlast3m_392P', 'maxdpdtolerance_374P', 'maxdbddpdlast1m_3658939P', 'maxdbddpdtollast12m_3658940P', 'maxdbddpdtollast6m_4187119P', 'maxdpdinstlnum_3546846P', 'maxlnamtstart6m_4525199A', 'maxoutstandbalancel12m_4187113A', 'numinstpaidearly_338L', 'numinstpaidearly5d_1087L', 'numinstpaidlate1d_3546852L', 'numincomingpmts_3546848L', 'numinstlsallpaid_934L', 'numinstlswithdpd10_728L', 'numinstlswithoutdpd_562L', 'numinstpaid_4499208L', 'numinstpaidearly3d_3546850L', 'numinstregularpaidest_4493210L', 'numinstpaidearly5dest_4493211L', 'sumoutstandtotalest_4493215A', 'numinstpaidlastcontr_4325080L', 'numinstregularpaid_973L', 'max_num_group1', 'max_num_group2_14', 'pctinstlsallpaidearl3d_427L', 'pctinstlsallpaidlate1d_3546856L', 'pctinstlsallpaidlat10d_839L', 'pctinstlsallpaidlate4d_3546849L', 'pctinstlsallpaidlate6d_3546844L', 'pmtnum_254L', 'posfpd10lastmonth_333P', 'posfpd30lastmonth_3976960P', 'posfstqpd30lastmonth_3976962P', 'price_1097A', 'sumoutstandtotal_3546847A', 'totaldebt_9A', 'mean_actualdpd_943P', 'mean_annuity_853A', 'mean_credacc_credlmt_575A', 'mean_credamount_590A', 'mean_downpmt_134A', 'mean_currdebt_94A', 'mean_mainoccupationinc_437A', 'mean_maxdpdtolerance_577P', 'mean_outstandingdebt_522A', 'max_byoccupationinc_3656910L', 'max_childnum_21L', 'max_pmtnum_8L', 'mean_amount_4527230A', 'max_num_group1_3', 'mean_amount_4917619A', 'max_num_group1_4', 'mean_pmtamount_36A', 'max_num_group1_5', 'mean_credlmt_230A', 'mean_credlmt_935A', 'mean_dpdmax_139P', 'max_dpdmaxdatemonth_89T', 'max_dpdmaxdateyear_596T', 'mean_dpdmax_757P', 'max_dpdmaxdatemonth_442T', 'max_dpdmaxdateyear_896T', 'mean_pmts_dpd_303P', 'mean_instlamount_768A', 'mean_monthlyinstlamount_332A', 'mean_monthlyinstlamount_674A', 'mean_outstandingamount_354A', 'mean_outstandingamount_362A', 'mean_overdueamount_31A', 'mean_overdueamount_659A', 'max_numberofoverdueinstls_725L', 'mean_overdueamountmax2_14A', 'mean_totaldebtoverduevalue_178A', 'mean_totaloutstanddebtvalue_39A', 'max_numberofcontrsvalue_258L', 'max_numberofoverdueinstlmax_1039L', 'max_overdueamountmaxdatemonth_365T', 'max_overdueamountmaxdateyear_2T', 'mean_pmts_overdue_1140A', 'max_pmts_month_158T', 'max_pmts_year_1139T', 'mean_overdueamountmax2_398A', 'max_numberofoverdueinstlmax_1151L', 'mean_overdueamountmax_35A', 'max_overdueamountmaxdatemonth_284T', 'max_overdueamountmaxdateyear_994T', 'mean_pmts_overdue_1152A', 'mean_residualamount_488A', 'mean_residualamount_856A', 'mean_totalamount_6A', 'mean_totalamount_996A', 'mean_totaldebtoverduevalue_718A', 'mean_totaloutstanddebtvalue_668A', 'max_numberofcontrsvalue_358L', 'max_nominalrate_281L', 'max_nominalrate_498L', 'max_numberofinstls_229L', 'max_numberofinstls_320L', 'max_numberofoutstandinstls_520L', 'max_numberofoutstandinstls_59L', 'max_numberofoverdueinstls_834L', 'max_periodicityofpmts_1102L', 'max_periodicityofpmts_837L', 'max_num_group1_6', 'mean_debtvalue_227A', 'mean_credlmt_1052A', 'mean_credlmt_3940954A', 'mean_residualamount_127A', 'mean_credlmt_228A', 'mean_debtpastduevalue_732A', 'mean_pmtdaysoverdue_1135P', 'mean_dpd_550P', 'mean_installmentamount_833A', 'mean_totalamount_503A', 'max_num_group1_12', 'max_num_group2', 'mean_installmentamount_644A', 'mean_totalamount_881A', 'max_credquantity_984L', 'mean_dpdmax_851P', 'mean_overdueamountmax_950A', 'max_dpdmaxdatemonth_804T', 'max_dpdmaxdateyear_742T', 'mean_pmts_dpdvalue_108P', 'mean_instlamount_892A', 'max_numberofinstls_810L', 'mean_residualamount_1093A', 'mean_residualamount_3940956A', 'max_interestrateyearly_538L', 'max_pmtnumpending_403L', 'max_num_group1_7', 'mean_amtdebitoutgoing_4809440A', 'mean_amtdepositbalance_4809441A', 'mean_amtdepositoutgoing_4809442A', 'max_num_group1_8', 'mean_amount_416A', 'max_num_group1_10', 'max_num_group1_11', 'max_collater_valueofguarantee_1124L', 'max_collater_valueofguarantee_876L', 'max_pmts_month_706T', 'max_pmts_year_507T', 'max_num_group1_13', 'max_num_group2_13', 'max_num_group1_15', 'max_num_group2_15', 'birthdate_574D', 'dateofbirth_337D', 'responsedate_1012D', 'responsedate_4527233D', 'datefirstoffer_1144D', 'datelastunpaid_3546854D', 'dtlastpmtallstes_4499206D', 'firstclxcampaign_1125D', 'firstdatedue_489D', 'lastactivateddate_801D', 'lastapplicationdate_877D', 'lastapprdate_640D', 'lastdelinqdate_224D', 'lastrejectdate_50D', 'maxdpdinstldate_3546855D', 'max_approvaldate_319D', 'max_creationdate_885D', 'max_dateactivated_425D', 'max_dtlastpmt_581D', 'max_dtlastpmtallstes_3545839D', 'max_employedfrom_700D', 'max_firstnonzeroinstldate_307D', 'max_recorddate_4527225D', 'max_deductiondate_4917603D', 'max_processingdate_168D', 'max_dateofcredend_289D', 'max_dateofcredend_353D', 'max_dateofcredstart_181D', 'max_dateofcredstart_739D', 'max_dateofrealrepmt_138D', 'max_lastupdate_1112D', 'max_lastupdate_388D', 'max_numberofoverdueinstlmaxdat_148D', 'max_numberofoverdueinstlmaxdat_641D', 'max_overdueamountmax2date_1002D', 'max_overdueamountmax2date_1142D', 'max_refreshdate_3813885D', 'max_contractdate_551D', 'max_contractmaturitydate_151D', 'max_lastupdate_260D', 'max_birth_259D', 'max_empl_employedfrom_271D', 'max_contractenddate_991D', 'max_openingdate_313D', 'max_openingdate_857D', 'max_pmts_date_1107D']\n"
     ]
    }
   ],
   "source": [
    "cat_cols = []\n",
    "num_cols = []\n",
    "\n",
    "for key,value in df_base.schema.items():\n",
    "    if str(value) in ['String','Date','Boolean']:\n",
    "        cat_cols.append(key)\n",
    "\n",
    "print('cat_cols',cat_cols)\n",
    "print('len of cat_cols',len(cat_cols))\n",
    "\n",
    "df_base = df_base.drop(cat_cols)\n",
    "num_cols = df_base.columns\n",
    "\n",
    "print('num_cols',num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of class 0: 0.97\n",
      "Proportion of class 1: 0.03\n",
      "\n",
      "\n",
      "X_train (1144994, 269)\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's auc: 0.778239\n",
      "[400]\tvalid_0's auc: 0.781152\n",
      "Early stopping, best iteration is:\n",
      "[368]\tvalid_0's auc: 0.781559\n",
      "CV AUC scores:  [0.7815589164437025]\n",
      "Maximum CV AUC score:  0.7815589164437025\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_base_pandas = df_base.to_pandas()\n",
    "\n",
    "\n",
    "df_base_pandas_y = df_base_pandas[['target']]\n",
    "df_base_pandas_X = df_base_pandas.drop(['target'], axis=1)\n",
    "\n",
    "# print(df_base_pandas_y.shape)\n",
    "# print(df_base_pandas_X.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "proportion(df_base_pandas_y)\n",
    "\n",
    "for train_indices, valid_indices in skf.split(df_base_pandas_X, df_base_pandas_y):\n",
    "    X_train = df_base_pandas_X.iloc[train_indices]\n",
    "    y_train = df_base_pandas_y.iloc[train_indices]\n",
    "    \n",
    "    X_valid = df_base_pandas_X.iloc[valid_indices]\n",
    "    y_valid = df_base_pandas_y.iloc[valid_indices]\n",
    "\n",
    "    print('X_train',X_train.shape)\n",
    "\n",
    "    # #resampling using smote\n",
    "    # X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "    # X_valid_resampled, y_valid_resampled = smote.fit_resample(X_valid, y_valid)\n",
    "\n",
    "    #resampling using adasyn\n",
    "    # X_train_resampled, y_train_resampled = adasyn.fit_resample(X_train, y_train)\n",
    "    # X_valid_resampled, y_valid_resampled = adasyn.fit_resample(X_valid, y_valid)\n",
    "\n",
    "    #resampling using random oversampler\n",
    "    # X_train_resampled, y_train_resampled = random_oversampler.fit_resample(X_train, y_train)\n",
    "    # X_valid_resampled, y_valid_resampled = random_oversampler.fit_resample(X_valid, y_valid)\n",
    "\n",
    "    # # without resampling\n",
    "    X_train_resampled, y_train_resampled = X_train, y_train\n",
    "    X_valid_resampled, y_valid_resampled = X_valid, y_valid\n",
    "\n",
    "\n",
    "    \n",
    "    if len(y_train_resampled.shape) > 1:\n",
    "        y_train_resampled = np.ravel(y_train_resampled)\n",
    "\n",
    "    if len(y_valid_resampled.shape) > 1:\n",
    "        y_valid_resampled = np.ravel(y_valid_resampled)\n",
    "\n",
    "\n",
    "    # proportion(y_train_resampled)\n",
    "    # proportion(y_valid)\n",
    "\n",
    "    # print('train_indices',len(train_indices))\n",
    "    # print('valid_indices', len(valid_indices))\n",
    "\n",
    "    #light gbm initialization\n",
    "    model = lgb.LGBMClassifier(**params_lgbm)\n",
    "\n",
    "    #training the model\n",
    "    model.fit(\n",
    "        X_train_resampled, y_train_resampled,\n",
    "        eval_set = [(X_valid_resampled, y_valid_resampled)],\n",
    "        callbacks = [lgb.log_evaluation(200), lgb.early_stopping(100)] )\n",
    "    \n",
    "    #adding models to the list\n",
    "    fitted_models.append(model)\n",
    "\n",
    "    y_pred_valid_resampled = model.predict_proba(X_valid_resampled)[:,1]\n",
    "    auc_score = roc_auc_score(y_valid_resampled, y_pred_valid_resampled)\n",
    "    cv_scores.append(auc_score)\n",
    "\n",
    "    break #remove later\n",
    "\n",
    "\n",
    "\n",
    "print(\"CV AUC scores: \", cv_scores)\n",
    "print(\"Maximum CV AUC score: \", max(cv_scores))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# del df_base_pandas, df_base_pandas_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LGBMClassifier(colsample_bynode=0.8, colsample_bytree=0.8, device='cpu',\n",
       "                extra_trees=True, learning_rate=0.05, max_depth=10, metric='auc',\n",
       "                n_estimators=2000, num_leaves=64, objective='binary',\n",
       "                random_state=42, reg_alpha=0.1, reg_lambda=10, verbose=-1)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STORE THE MODEL IN A GIVEN PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\Semester-2\\DS ML\\Group-Project\\Data_Home_Credit\\parquet_files\\train_inter\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['E:\\\\Semester-2\\\\DS ML\\\\Group-Project\\\\Data_Home_Credit\\\\parquet_files\\\\train_inter/lgbm_model_.joblib']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stroing the trained model\n",
    "\n",
    "from joblib import dump\n",
    "\n",
    "print(TRAIN_INTER)\n",
    "\n",
    "dump(fitted_models, f'{TRAIN_INTER}/lgbm_model_.joblib')\n",
    "\n",
    "# for i, model in enumerate(fitted_models):\n",
    "#     dump(model, f'{TRAIN_INTER}/model_{i}.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model type: <class 'lightgbm.sklearn.LGBMClassifier'>\n",
      "feature_name_: ['credamount_770A', 'applicationcnt_361L', 'applications30d_658L', 'applicationscnt_1086L', 'applicationscnt_464L', 'applicationscnt_867L', 'clientscnt_1022L', 'clientscnt_100L', 'clientscnt_1071L', 'clientscnt_1130L', 'clientscnt_157L', 'clientscnt_257L', 'clientscnt_304L', 'clientscnt_360L', 'clientscnt_493L', 'clientscnt_533L', 'clientscnt_887L', 'clientscnt_946L', 'deferredmnthsnum_166L', 'disbursedcredamount_1113A', 'downpmt_116A', 'homephncnt_628L', 'mobilephncnt_593L', 'numactivecreds_622L', 'numactivecredschannel_414L', 'numactiverelcontr_750L', 'numcontrs3months_479L', 'numnotactivated_1143L', 'numpmtchanneldd_318L', 'numrejects9m_859L', 'sellerplacecnt_915L', 'mean_mainoccupationinc_384A', 'max_num_group1_9', 'days180_256L', 'days30_165L', 'days360_512L', 'firstquarter_103L', 'fourthquarter_440L', 'secondquarter_766L', 'thirdquarter_1082L', 'mean_debtoutstand_525A', 'mean_debtoverdue_47A', 'pmtscount_423L', 'pmtssum_45A', 'actualdpdtolerance_344P', 'amtinstpaidbefduel24m_4187115A', 'numinstlswithdpd5_4187116L', 'annuitynextmonth_57A', 'currdebt_22A', 'currdebtcredtyperange_828A', 'numinstls_657L', 'totalsettled_863A', 'mindbddpdlast24m_3658935P', 'avgdbddpdlast3m_4187120P', 'avgdbdtollast24m_4525197P', 'avgdpdtolclosure24_3658938P', 'avginstallast24m_3658937A', 'maxinstallast24m_3658928A', 'avgmaxdpdlast9m_3716943P', 'avgoutstandbalancel6m_4187114A', 'avgpmtlast12m_4525200A', 'cntincpaycont9m_3716944L', 'cntpmts24_3658933L', 'commnoinclast6m_3546845L', 'maxdpdfrom6mto36m_3546853P', 'daysoverduetolerancedd_3976961L', 'numinsttopaygr_769L', 'eir_270L', 'interestrate_311L', 'lastapprcredamount_781A', 'lastrejectcredamount_222A', 'maininc_215A', 'mastercontrelectronic_519L', 'mastercontrexist_109L', 'maxannuity_159A', 'maxdebt4_972A', 'maxdpdlast24m_143P', 'maxdpdlast3m_392P', 'maxdpdtolerance_374P', 'maxdbddpdlast1m_3658939P', 'maxdbddpdtollast12m_3658940P', 'maxdbddpdtollast6m_4187119P', 'maxdpdinstlnum_3546846P', 'maxlnamtstart6m_4525199A', 'maxoutstandbalancel12m_4187113A', 'numinstpaidearly_338L', 'numinstpaidearly5d_1087L', 'numinstpaidlate1d_3546852L', 'numincomingpmts_3546848L', 'numinstlsallpaid_934L', 'numinstlswithdpd10_728L', 'numinstlswithoutdpd_562L', 'numinstpaid_4499208L', 'numinstpaidearly3d_3546850L', 'numinstregularpaidest_4493210L', 'numinstpaidearly5dest_4493211L', 'sumoutstandtotalest_4493215A', 'numinstpaidlastcontr_4325080L', 'numinstregularpaid_973L', 'max_num_group1', 'max_num_group2_14', 'pctinstlsallpaidearl3d_427L', 'pctinstlsallpaidlate1d_3546856L', 'pctinstlsallpaidlat10d_839L', 'pctinstlsallpaidlate4d_3546849L', 'pctinstlsallpaidlate6d_3546844L', 'pmtnum_254L', 'posfpd10lastmonth_333P', 'posfpd30lastmonth_3976960P', 'posfstqpd30lastmonth_3976962P', 'price_1097A', 'sumoutstandtotal_3546847A', 'totaldebt_9A', 'mean_actualdpd_943P', 'mean_annuity_853A', 'mean_credacc_credlmt_575A', 'mean_credamount_590A', 'mean_downpmt_134A', 'mean_currdebt_94A', 'mean_mainoccupationinc_437A', 'mean_maxdpdtolerance_577P', 'mean_outstandingdebt_522A', 'max_byoccupationinc_3656910L', 'max_childnum_21L', 'max_pmtnum_8L', 'mean_amount_4527230A', 'max_num_group1_3', 'mean_amount_4917619A', 'max_num_group1_4', 'mean_pmtamount_36A', 'max_num_group1_5', 'mean_credlmt_230A', 'mean_credlmt_935A', 'mean_dpdmax_139P', 'max_dpdmaxdatemonth_89T', 'max_dpdmaxdateyear_596T', 'mean_dpdmax_757P', 'max_dpdmaxdatemonth_442T', 'max_dpdmaxdateyear_896T', 'mean_pmts_dpd_303P', 'mean_instlamount_768A', 'mean_monthlyinstlamount_332A', 'mean_monthlyinstlamount_674A', 'mean_outstandingamount_354A', 'mean_outstandingamount_362A', 'mean_overdueamount_31A', 'mean_overdueamount_659A', 'max_numberofoverdueinstls_725L', 'mean_overdueamountmax2_14A', 'mean_totaldebtoverduevalue_178A', 'mean_totaloutstanddebtvalue_39A', 'max_numberofcontrsvalue_258L', 'max_numberofoverdueinstlmax_1039L', 'max_overdueamountmaxdatemonth_365T', 'max_overdueamountmaxdateyear_2T', 'mean_pmts_overdue_1140A', 'max_pmts_month_158T', 'max_pmts_year_1139T', 'mean_overdueamountmax2_398A', 'max_numberofoverdueinstlmax_1151L', 'mean_overdueamountmax_35A', 'max_overdueamountmaxdatemonth_284T', 'max_overdueamountmaxdateyear_994T', 'mean_pmts_overdue_1152A', 'mean_residualamount_488A', 'mean_residualamount_856A', 'mean_totalamount_6A', 'mean_totalamount_996A', 'mean_totaldebtoverduevalue_718A', 'mean_totaloutstanddebtvalue_668A', 'max_numberofcontrsvalue_358L', 'max_nominalrate_281L', 'max_nominalrate_498L', 'max_numberofinstls_229L', 'max_numberofinstls_320L', 'max_numberofoutstandinstls_520L', 'max_numberofoutstandinstls_59L', 'max_numberofoverdueinstls_834L', 'max_periodicityofpmts_1102L', 'max_periodicityofpmts_837L', 'max_num_group1_6', 'mean_debtvalue_227A', 'mean_credlmt_1052A', 'mean_credlmt_3940954A', 'mean_residualamount_127A', 'mean_credlmt_228A', 'mean_debtpastduevalue_732A', 'mean_pmtdaysoverdue_1135P', 'mean_dpd_550P', 'mean_installmentamount_833A', 'mean_totalamount_503A', 'max_num_group1_12', 'max_num_group2', 'mean_installmentamount_644A', 'mean_totalamount_881A', 'max_credquantity_984L', 'mean_dpdmax_851P', 'mean_overdueamountmax_950A', 'max_dpdmaxdatemonth_804T', 'max_dpdmaxdateyear_742T', 'mean_pmts_dpdvalue_108P', 'mean_instlamount_892A', 'max_numberofinstls_810L', 'mean_residualamount_1093A', 'mean_residualamount_3940956A', 'max_interestrateyearly_538L', 'max_pmtnumpending_403L', 'max_num_group1_7', 'mean_amtdebitoutgoing_4809440A', 'mean_amtdepositbalance_4809441A', 'mean_amtdepositoutgoing_4809442A', 'max_num_group1_8', 'mean_amount_416A', 'max_num_group1_10', 'max_num_group1_11', 'max_collater_valueofguarantee_1124L', 'max_collater_valueofguarantee_876L', 'max_pmts_month_706T', 'max_pmts_year_507T', 'max_num_group1_13', 'max_num_group2_13', 'max_num_group1_15', 'max_num_group2_15', 'birthdate_574D', 'dateofbirth_337D', 'responsedate_1012D', 'responsedate_4527233D', 'datefirstoffer_1144D', 'datelastunpaid_3546854D', 'dtlastpmtallstes_4499206D', 'firstclxcampaign_1125D', 'firstdatedue_489D', 'lastactivateddate_801D', 'lastapplicationdate_877D', 'lastapprdate_640D', 'lastdelinqdate_224D', 'lastrejectdate_50D', 'maxdpdinstldate_3546855D', 'max_approvaldate_319D', 'max_creationdate_885D', 'max_dateactivated_425D', 'max_dtlastpmt_581D', 'max_dtlastpmtallstes_3545839D', 'max_employedfrom_700D', 'max_firstnonzeroinstldate_307D', 'max_recorddate_4527225D', 'max_deductiondate_4917603D', 'max_processingdate_168D', 'max_dateofcredend_289D', 'max_dateofcredend_353D', 'max_dateofcredstart_181D', 'max_dateofcredstart_739D', 'max_dateofrealrepmt_138D', 'max_lastupdate_1112D', 'max_lastupdate_388D', 'max_numberofoverdueinstlmaxdat_148D', 'max_numberofoverdueinstlmaxdat_641D', 'max_overdueamountmax2date_1002D', 'max_overdueamountmax2date_1142D', 'max_refreshdate_3813885D', 'max_contractdate_551D', 'max_contractmaturitydate_151D', 'max_lastupdate_260D', 'max_birth_259D', 'max_empl_employedfrom_271D', 'max_contractenddate_991D', 'max_openingdate_313D', 'max_openingdate_857D', 'max_pmts_date_1107D']\n",
      "269\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Load the model\n",
    "model = joblib.load(TRAIN_INTER / 'lgbm_model_.joblib')\n",
    "\n",
    "# print(model)\n",
    "# Now you can use the loaded model for predictions or other tasks\n",
    "\n",
    "cols_used_for_training = []\n",
    "\n",
    "for i in model:\n",
    "    \n",
    "    # Print type of model\n",
    "    print(\"Model type:\", type(i))\n",
    "\n",
    "    # Print model attributes and methods\n",
    "    # print(\"Model attributes and methods:\", dir(i))\n",
    "\n",
    "    \n",
    "    # get the list of feature names on which the model is trained\n",
    "    print(\"feature_name_:\", i.feature_name_)\n",
    "\n",
    "    cols_used_for_training= i.feature_name_\n",
    "\n",
    "\n",
    "    print(len(i.feature_name_))\n",
    "\n",
    "        \n",
    "    break\n",
    "\n",
    "fitted_models = model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VOTING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "class VotingModel(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, estimators):\n",
    "        super().__init__()\n",
    "        self.estimators = estimators\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_preds = [estimator.predict(X) for estimator in self.estimators]\n",
    "        mode_result = mode(y_preds).mode\n",
    "        return y_preds[0]\n",
    "     \n",
    "    def predict_proba(self, X):      \n",
    "        # lgb\n",
    "        y_preds = [estimator.predict_proba(X) for estimator in self.estimators[:5]]\n",
    "        \n",
    "        # # cat        \n",
    "        # X[cat_cols] = X[cat_cols].astype(str)\n",
    "        # y_preds += [estimator.predict_proba(X) for estimator in self.estimators[-5:]]\n",
    "        \n",
    "        return np.mean(y_preds, axis=0)\n",
    "    \n",
    "model = VotingModel(fitted_models)\n",
    "\n",
    "print(len(model.estimators))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "READ THE PARQUET FILE (TRAINING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the parquet file\n",
    "filepath = TRAIN_INTER / \"joined.parquet\"\n",
    "df_base = pl.read_parquet(filepath)\n",
    "\n",
    "# df_base = df_base.pipe(Pipeline.encode_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPLIT THE DATA USING SHUFFLE SPLIT TO TEST THE GINI STABILITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train (1221327, 269)\n",
      "Shape of X_valid (183199, 269)\n",
      "Shape of X_test (122133, 269)\n",
      "Index(['credamount_770A', 'applicationcnt_361L', 'applications30d_658L',\n",
      "       'applicationscnt_1086L', 'applicationscnt_464L', 'applicationscnt_867L',\n",
      "       'clientscnt_1022L', 'clientscnt_100L', 'clientscnt_1071L',\n",
      "       'clientscnt_1130L',\n",
      "       ...\n",
      "       'max_refreshdate_3813885D', 'max_contractdate_551D',\n",
      "       'max_contractmaturitydate_151D', 'max_lastupdate_260D',\n",
      "       'max_birth_259D', 'max_empl_employedfrom_271D',\n",
      "       'max_contractenddate_991D', 'max_openingdate_313D',\n",
      "       'max_openingdate_857D', 'max_pmts_date_1107D'],\n",
      "      dtype='object', length=269)\n"
     ]
    }
   ],
   "source": [
    "def getPandasdata_forIds(df, case_ids):\n",
    "    df = df.filter(pl.col(\"case_id\").is_in(case_ids))\n",
    "    return (\n",
    "        df.filter(pl.col(\"case_id\").is_in(case_ids))[[\"case_id\", \"WEEK_NUM\", 'target']].to_pandas(),\n",
    "        df[cols_used_for_training].select(pl.exclude(['target'])).to_pandas(),\n",
    "        df[['target']].to_pandas()\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "#convert series data to numpy array, in order to send to test_train_split\n",
    "case_ids = df_base[\"case_id\"].unique().shuffle(seed=1).to_numpy()\n",
    "\n",
    "\n",
    "case_ids_train, case_ids_test = train_test_split(case_ids, train_size=0.8, random_state=1)\n",
    "case_ids_valid, case_ids_test = train_test_split(case_ids_test, train_size=0.6, random_state=1)\n",
    "\n",
    "#test the data by following the starter notebook\n",
    "base_train, X_train, y_train = getPandasdata_forIds(df_base, case_ids_train)\n",
    "base_valid, X_valid, y_valid = getPandasdata_forIds(df_base, case_ids_valid)\n",
    "base_test, X_test, y_test = getPandasdata_forIds(df_base, case_ids_test)\n",
    "\n",
    "print('Shape of X_train',X_train.shape)\n",
    "print('Shape of X_valid',X_valid.shape)\n",
    "print('Shape of X_test',X_test.shape)\n",
    "\n",
    "print(X_train.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'target'}\n"
     ]
    }
   ],
   "source": [
    "print(set(num_cols)-set(X_train.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREDICTION FROM THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{dtype('float64'), dtype('int8'), dtype('int64'), dtype('int32'), dtype('int16')}\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# print(X_train.dtypes)\n",
    "s = set()\n",
    "l = []\n",
    "for key,value in X_train.dtypes.items():\n",
    "    s.add(value)\n",
    "    if value in ['datetime64[ms]']:\n",
    "        l.append(key)\n",
    "\n",
    "print(s)\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for base, X in [(base_train, X_train), (base_valid, X_valid), (base_test, X_test)]:\n",
    "    y_pred = model.predict_proba(X)[:,1]\n",
    "    base[\"score\"] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base.test Index(['case_id', 'WEEK_NUM', 'target', 'score'], dtype='object')\n",
      "y_test Index(['target'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print('base.test',base_test.columns)\n",
    "print('y_test',y_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC AND AUC CURVE FOR THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC score on the train set is: 0.8607784409786201\n",
      "The AUC score on the valid set is: 0.8572637854566031\n",
      "The AUC score on the test set is: 0.8584569327608836\n"
     ]
    }
   ],
   "source": [
    "print(f'The AUC score on the train set is: {roc_auc_score(y_train[\"target\"], base_train[\"score\"])}') \n",
    "print(f'The AUC score on the valid set is: {roc_auc_score(y_valid[\"target\"], base_valid[\"score\"])}') \n",
    "print(f'The AUC score on the test set is: {roc_auc_score(y_test[\"target\"], base_test[\"score\"])}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "0    1182976\n",
      "1      38351\n",
      "Name: count, dtype: int64\n",
      "target\n",
      "0    177496\n",
      "1      5703\n",
      "Name: count, dtype: int64\n",
      "target\n",
      "0    118193\n",
      "1      3940\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(base_train['target'].value_counts())\n",
    "print(base_valid['target'].value_counts())\n",
    "print(base_test['target'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GINI SCORE DEFINITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_stability(base, w_fallingrate=88.0, w_resstd=-0.5):\n",
    "    gini_in_time = base.loc[:, [\"WEEK_NUM\", \"target\", \"score\"]]\\\n",
    "    .sort_values(\"WEEK_NUM\")\\\n",
    "    .groupby(\"WEEK_NUM\")[[\"target\", \"score\"]]\\\n",
    "    .apply(lambda x: 2*roc_auc_score(x[\"target\"], x[\"score\"])-1 if len(x[\"target\"].unique()) > 1 else 0).tolist()\n",
    "    \n",
    "\n",
    "    # print(base)\n",
    "\n",
    "    # for group_name, group_data in updated_base:\n",
    "    #     print(f\"Group: {group_name}\")\n",
    "    #     print(group_data)\n",
    "    #     print() \n",
    "\n",
    "    # print(base['target'].value_counts())\n",
    "\n",
    "    gini_in_time = [x for x in gini_in_time if x > 0]\n",
    "\n",
    "    # print(gini_in_time)\n",
    "    x = np.arange(len(gini_in_time))\n",
    "    y = gini_in_time\n",
    "    a, b = np.polyfit(x, y, 1)\n",
    "    y_hat = a*x + b\n",
    "    residuals = y - y_hat\n",
    "    res_std = np.std(residuals)\n",
    "    avg_gini = np.mean(gini_in_time)\n",
    "    return avg_gini + w_fallingrate * min(0, a) + w_resstd * res_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMPUTE THE GINI SCORES FOR THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stability score on the train set is: 0.7024239236847906\n",
      "The stability score on the valid set is: 0.6826155517694663\n",
      "The stability score on the test set is: 0.6820645269280128\n"
     ]
    }
   ],
   "source": [
    "stability_score_train = gini_stability(base_train)\n",
    "stability_score_valid = gini_stability(base_valid)\n",
    "stability_score_test = gini_stability(base_test)\n",
    "\n",
    "print(f'The stability score on the train set is: {stability_score_train}') \n",
    "print(f'The stability score on the valid set is: {stability_score_valid}') \n",
    "print(f'The stability score on the test set is: {stability_score_test}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=================== TEST DATA PROCESSING STARTS FROM HERE ================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE DATA STORE FOR TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_store_test(path):\n",
    "    d = {\n",
    "        \"df_base\": [\n",
    "            [read_file , path / \"test_base.parquet\"],\n",
    "        ],\n",
    "\n",
    "        \"depth_0\": [\n",
    "            [read_file , path / \"test_static_cb_0.parquet\"],\n",
    "            [read_files , path / \"test_static_0_*.parquet\"],\n",
    "        ],\n",
    "\n",
    "        \"depth_1\": [\n",
    "\n",
    "            [read_files , path / \"test_applprev_1_*.parquet\", 1],\n",
    "            [read_file , path / \"test_tax_registry_a_1.parquet\", 1],\n",
    "            [read_file , path / \"test_tax_registry_b_1.parquet\", 1],\n",
    "            [read_file , path / \"test_tax_registry_c_1.parquet\", 1],\n",
    "            [read_files , path / \"test_credit_bureau_a_1_*.parquet\", 1],\n",
    "            [read_file , path / \"test_credit_bureau_b_1.parquet\", 1],\n",
    "            [read_file , path / \"test_other_1.parquet\", 1],\n",
    "            [read_file , path / \"test_person_1.parquet\", 1],\n",
    "            [read_file , path / \"test_deposit_1.parquet\", 1],\n",
    "            [read_file , path / \"test_debitcard_1.parquet\", 1],\n",
    "        ],\n",
    "\n",
    "\n",
    "        \"depth_2\": [\n",
    "            [read_file , path / \"test_credit_bureau_b_2.parquet\", 2],\n",
    "            [read_files , path / \"test_credit_bureau_a_2_*.parquet\", 2],\n",
    "            [read_file , path / \"test_applprev_2.parquet\", 2],\n",
    "            [read_file , path / \"test_person_2.parquet\", 2],\n",
    "        ]\n",
    "\n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the data store format for the given test data \n",
    "data_store = create_data_store_test(TEST_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UPDATEPARQUETFILES DEFINITION FOR TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def updateParquetFiles_test(data_store,cols_must_present=[]):\n",
    "    for value in data_store.values():\n",
    "        for item in value:\n",
    "            # print(item[1]) -- corresponds to the path of the parquet_file\n",
    "\n",
    "            if len(item)==2:\n",
    "                df = item[0](item[1]) #readfile/ readfiles happens here\n",
    "            elif len(item)==3:\n",
    "                df = item[0](item[1],item[2]) #readfile/ readfiles happens here\n",
    "            \n",
    "            #skip the empty file\n",
    "            if df.shape[0] == 0:\n",
    "                continue\n",
    "\n",
    "            #reduce the dataframe memory size\n",
    "            df = df.pipe(reduce_mem_usage_polars)\n",
    "\n",
    "            #WRITE df as parquet file to ../train_inter/ directory.\n",
    "            # 1. Get the filename from the path\n",
    "            filename = os.path.basename(item[1])\n",
    "            filename = filename.replace(\"_*\",\"\")\n",
    "            # # Get the folder name from the path\n",
    "            folder_name = os.path.basename(os.path.dirname(item[1]))\n",
    "\n",
    "            # print(folder_name)\n",
    "\n",
    "            # 2. Go one folder back\n",
    "            parent_directory = os.path.dirname(os.path.dirname(item[1]))\n",
    "\n",
    "            # 3. Create directory '{folder_name}_inter1' if not present\n",
    "            new_directory = os.path.join(parent_directory, f\"{folder_name}_inter1\")\n",
    "\n",
    "            \n",
    "\n",
    "            if not os.path.exists(new_directory):\n",
    "                os.makedirs(new_directory)\n",
    "\n",
    "            # 4. Create new path\n",
    "            new_path = os.path.join(new_directory, filename)\n",
    "\n",
    "            # Write the DataFrame to a Parquet file in the new path\n",
    "            df.write_parquet(new_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UPDATE TEST PARQUET FILES TO TEST1 FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 0.00 MB\n",
      "Memory usage after optimization is: 0.00 MB\n",
      "Decreased by 53.6%\n",
      "\n",
      "\n",
      "Memory usage of dataframe is 0.00 MB\n",
      "heyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\n",
      "heyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\n",
      "heyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\n",
      "heyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\n",
      "heyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\n",
      "heyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\n",
      "heyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\n",
      "heyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\n",
      "heyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\n",
      "heyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\n",
      "heyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\n",
      "heyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\n",
      "heyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\n",
      "heyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\n",
      "heyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\n",
      "heyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\n",
      "heyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\n",
      "heyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\n",
      "heyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\n",
      "heyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\n",
      "heyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\n",
      "heyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\n",
      "heyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\n",
      "Memory usage after optimization is: 0.00 MB\n",
      "Decreased by 24.0%\n",
      "\n",
      "\n",
      "Memory usage of dataframe is 0.03 MB\n",
      "heyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\n",
      "heyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\n",
      "heyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\n",
      "heyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\n",
      "heyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\n",
      "heyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\n",
      "Memory usage after optimization is: 0.01 MB\n",
      "Decreased by 63.6%\n",
      "\n",
      "\n",
      "Memory usage of dataframe is 0.00 MB\n",
      "Memory usage after optimization is: 0.00 MB\n",
      "Decreased by 44.1%\n",
      "\n",
      "\n",
      "Memory usage of dataframe is 0.00 MB\n",
      "Memory usage after optimization is: 0.00 MB\n",
      "Decreased by 47.2%\n",
      "\n",
      "\n",
      "Memory usage of dataframe is 0.00 MB\n",
      "Memory usage after optimization is: 0.00 MB\n",
      "Decreased by 47.2%\n",
      "\n",
      "\n",
      "Memory usage of dataframe is 0.00 MB\n",
      "heyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\n",
      "heyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\n",
      "Memory usage after optimization is: 0.00 MB\n",
      "Decreased by 56.2%\n",
      "\n",
      "\n",
      "Memory usage of dataframe is 0.00 MB\n",
      "Memory usage after optimization is: 0.00 MB\n",
      "Decreased by 53.8%\n",
      "\n",
      "\n",
      "Memory usage of dataframe is 0.00 MB\n",
      "Memory usage after optimization is: 0.00 MB\n",
      "Decreased by 66.1%\n",
      "\n",
      "\n",
      "Memory usage of dataframe is 0.00 MB\n",
      "Memory usage after optimization is: 0.00 MB\n",
      "Decreased by 22.9%\n",
      "\n",
      "\n",
      "Memory usage of dataframe is 0.00 MB\n",
      "Memory usage after optimization is: 0.00 MB\n",
      "Decreased by 52.8%\n",
      "\n",
      "\n",
      "Memory usage of dataframe is 0.00 MB\n",
      "heyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\n",
      "heyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\n",
      "heyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\n",
      "Memory usage after optimization is: 0.00 MB\n",
      "Decreased by 24.7%\n",
      "\n",
      "\n",
      "Memory usage of dataframe is 0.00 MB\n",
      "Memory usage after optimization is: 0.00 MB\n",
      "Decreased by 72.7%\n",
      "\n",
      "\n",
      "Memory usage of dataframe is 0.00 MB\n",
      "Memory usage after optimization is: 0.00 MB\n",
      "Decreased by 49.5%\n",
      "\n",
      "\n",
      "Memory usage of dataframe is 0.00 MB\n",
      "Memory usage after optimization is: 0.00 MB\n",
      "Decreased by 39.1%\n",
      "\n",
      "\n",
      "Memory usage of dataframe is 0.00 MB\n",
      "Memory usage after optimization is: 0.00 MB\n",
      "Decreased by 24.2%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "updateParquetFiles_test(data_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FEATURE ENGINEERING DEFINTION FOR TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def feature_eng_test(df_base, depth_0, depth_1, depth_2):\n",
    "\n",
    "    # print(df_base)\n",
    "    folderPath = os.path.dirname(os.path.dirname(df_base[0][1]))\n",
    "    folderName = os.path.basename(os.path.dirname(df_base[0][1]))\n",
    "\n",
    "    outputFolderName = folderName.replace(\"inter1\",\"inter\")\n",
    "    outputFolderPath = os.path.join(folderPath,outputFolderName)\n",
    "\n",
    "    # print('outputFolderPath',outputFolderPath)\n",
    "\n",
    "\n",
    "    #reading the parquet file using polars\n",
    "    df_base = pl.read_parquet(df_base[0][1])\n",
    "\n",
    "    #adding new columns to df_base\n",
    "    df_base = df_base.with_columns(\n",
    "        month_decision = pl.col(\"date_decision\").dt.month().cast(pl.Int8),\n",
    "        weekday_decision = pl.col(\"date_decision\").dt.weekday().cast(pl.Int8),\n",
    "    )\n",
    "    # print(df_base)\n",
    "\n",
    "    for i, list1 in enumerate(depth_0 + depth_1 + depth_2):\n",
    "        filename = str(list1[1])\n",
    "        \n",
    "        filename = filename.replace(\"_*\",\"\")\n",
    "        # print(filename)\n",
    "\n",
    "        if os.path.exists(filename):\n",
    "            df = pl.read_parquet(filename)\n",
    "            df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")\n",
    "\n",
    "            \n",
    "        \n",
    "    outputFilename = os.path.join(outputFolderPath,'joined.parquet')\n",
    "    print('outputfilename',outputFilename)\n",
    "\n",
    "    #create director if not present\n",
    "    if not os.path.exists(outputFolderPath):\n",
    "        # Create the directory\n",
    "        os.makedirs(outputFolderPath)\n",
    "\n",
    "    df_base = df_base.pipe(Pipeline.handle_dates)\n",
    "\n",
    "    #writing the joined.parquet file to the outputFilename\n",
    "    df_base.write_parquet(outputFilename)\n",
    "\n",
    "    \n",
    "    return df_base\n",
    "\n",
    "    # print('size of df_base',df_base)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FEATURE ENGINEERING ON TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputfilename E:\\Semester-2\\DS ML\\Group-Project\\Data_Home_Credit\\parquet_files\\test_inter\\joined.parquet\n",
      "shape: (10, 482)\n",
      "┌─────────┬──────────┬────────────┬────────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
      "│ case_id ┆ WEEK_NUM ┆ month_deci ┆ weekday_de ┆ … ┆ max_addre ┆ max_relat ┆ max_num_g ┆ max_num_g │\n",
      "│ ---     ┆ ---      ┆ sion       ┆ cision     ┆   ┆ s_role_87 ┆ edpersons ┆ roup1_15  ┆ roup2_15  │\n",
      "│ i32     ┆ i8       ┆ ---        ┆ ---        ┆   ┆ 1L        ┆ _role_762 ┆ ---       ┆ ---       │\n",
      "│         ┆          ┆ i8         ┆ i8         ┆   ┆ ---       ┆ T         ┆ i8        ┆ i8        │\n",
      "│         ┆          ┆            ┆            ┆   ┆ str       ┆ ---       ┆           ┆           │\n",
      "│         ┆          ┆            ┆            ┆   ┆           ┆ str       ┆           ┆           │\n",
      "╞═════════╪══════════╪════════════╪════════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
      "│ 57543   ┆ 100      ┆ 5          ┆ 5          ┆ … ┆ null      ┆ null      ┆ null      ┆ null      │\n",
      "│ 57549   ┆ 100      ┆ 1          ┆ 1          ┆ … ┆ null      ┆ null      ┆ null      ┆ null      │\n",
      "│ 57551   ┆ 100      ┆ 11         ┆ 5          ┆ … ┆ null      ┆ null      ┆ 0         ┆ 0         │\n",
      "│ 57552   ┆ 100      ┆ 11         ┆ 5          ┆ … ┆ null      ┆ null      ┆ 0         ┆ 0         │\n",
      "│ 57569   ┆ 100      ┆ 12         ┆ 1          ┆ … ┆ TEMPORARY ┆ PARENT    ┆ 1         ┆ 5         │\n",
      "│ 57630   ┆ 100      ┆ 3          ┆ 2          ┆ … ┆ null      ┆ null      ┆ null      ┆ null      │\n",
      "│ 57631   ┆ 100      ┆ 6          ┆ 6          ┆ … ┆ null      ┆ null      ┆ null      ┆ null      │\n",
      "│ 57632   ┆ 100      ┆ 2          ┆ 6          ┆ … ┆ null      ┆ null      ┆ null      ┆ null      │\n",
      "│ 57633   ┆ 100      ┆ 1          ┆ 2          ┆ … ┆ null      ┆ null      ┆ null      ┆ null      │\n",
      "│ 57634   ┆ 100      ┆ 1          ┆ 3          ┆ … ┆ null      ┆ null      ┆ null      ┆ null      │\n",
      "└─────────┴──────────┴────────────┴────────────┴───┴───────────┴───────────┴───────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "df_base_submission = feature_eng_test(**create_data_store_test(TEST_INTER1))\n",
    "\n",
    "#keep columns in 'df' that are present while training the model\n",
    "#PENDING -- pass an list as angument, dont drop the columns, if that is present in list\n",
    "def keep_columns(df,cols_must_present=[]):\n",
    "    for col in df.columns:\n",
    "        if col not in cols_must_present:\n",
    "            df = df.drop(col)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# df_base_submission = keep_columns(df_base_submission,cols_must_present)\n",
    "\n",
    "\n",
    "# df_base_submission = df_base_submission.pipe(Pipeline.encode_cols)\n",
    "\n",
    "print(df_base_submission)\n",
    "\n",
    "# a = set(cols_must_present)\n",
    "# b= set(df_base_submission.schema.keys())\n",
    "\n",
    "# print(a-b)\n",
    "\n",
    "\n",
    "# for key,val in df_base.schema.items():\n",
    "    \n",
    "#     if str(val) in ['String']:\n",
    "#         print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEEK_NUM\n",
      "WEEK_NUM\n",
      "WEEK_NUM\n",
      "WEEK_NUM\n",
      "WEEK_NUM\n"
     ]
    }
   ],
   "source": [
    "# print(df_base_submission.dtypes)\n",
    "for index, col in enumerate(df_base_submission.columns):\n",
    "    # print(index, df_base_submission[col].dtype)\n",
    "    pass\n",
    "        \n",
    "        \n",
    "l1 = [48,52, 414,415, 469]\n",
    "\n",
    "l = df_base_submission.columns\n",
    "\n",
    "for i in l1:\n",
    "    print(l[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269\n",
      "missed columns {'max_num_group1_5', 'mean_pmtamount_36A', 'max_processingdate_168D'}\n",
      "num of cols before 267\n",
      "num of cols after 270\n"
     ]
    }
   ],
   "source": [
    "# selected_columns = num_cols\n",
    "\n",
    "# selected_columns = cols_used_for_training\n",
    "\n",
    "if 'target' in cols_used_for_training:\n",
    "    cols_used_for_training.remove('target')\n",
    "\n",
    "print(len(cols_used_for_training))\n",
    "\n",
    "print('missed columns',set(cols_used_for_training)-set(df_base_submission.columns))\n",
    "\n",
    "missed_cols = set(cols_used_for_training)-set(df_base_submission.columns)\n",
    "final_cols = list(set(cols_used_for_training)-set(missed_cols)) + ['case_id']\n",
    "\n",
    "df_base_submission_changed = df_base_submission.select(final_cols)\n",
    "\n",
    "# print(df_base_submission_changed)\n",
    "\n",
    "print('num of cols before',len(df_base_submission_changed.columns))\n",
    "\n",
    "for i in list(missed_cols):\n",
    "    df_base_submission_changed = df_base_submission_changed.with_columns(\n",
    "        pl.lit(None).cast(pl.Int8).alias(i)\n",
    "    )\n",
    "\n",
    "print('num of cols after',len(df_base_submission_changed.columns))\n",
    "\n",
    "# print(df_base_submission_changed[['mean_pmtamount_36A']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OUTPUT PREDICTION FOR TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            score\n",
      "case_id          \n",
      "57543    0.187879\n",
      "57549    0.155747\n",
      "57551    0.144672\n",
      "57552    0.124722\n",
      "57569    0.302953\n",
      "57630    0.136577\n",
      "57631    0.127497\n",
      "57632    0.186687\n",
      "57633    0.115593\n",
      "57634    0.150649\n"
     ]
    }
   ],
   "source": [
    "#predict the output score\n",
    "\n",
    "df_base_submission_pandas = df_base_submission_changed.to_pandas()\n",
    "df_base_submission_pandas = df_base_submission_pandas.set_index(\"case_id\")\n",
    "\n",
    "y_pred = pd.Series(model.predict_proba(df_base_submission_pandas)[:, 1], index=df_base_submission_pandas.index)\n",
    "\n",
    "df_subm = pd.read_csv(ROOT / \"sample_submission.csv\")\n",
    "df_subm = df_subm.set_index(\"case_id\")\n",
    "\n",
    "df_subm[\"score\"] = y_pred\n",
    "df_subm.to_csv(ROOT / \"submission_result.csv\")\n",
    "\n",
    "print(df_subm)\n",
    "\n",
    "# print(df_base_submission_pandas)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
